{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, pystan as ps, numpy.random as npr, matplotlib.pyplot as plt, h5py\n",
    "%matplotlib inline \n",
    "from time import time\n",
    "from pylab import plot, show, legend\n",
    "from scipy.stats import pearsonr, spearmanr, norm, invgamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile Stan model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_bf1f70352a45ebcc131d582eff912e1a NOW.\n"
     ]
    }
   ],
   "source": [
    "sm_gdP = ps.StanModel(file=\"gdP_logistic.stan\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = h5py.File(\"GZZ_data2.jld\", \"r\")\n",
    "X = data[\"X\"].value\n",
    "y = data[\"y\"].value\n",
    "両_true = data[\"xi_true\"].value\n",
    "d, Nobs = np.shape(X.transpose())\n",
    "\n",
    "data = dict(N=Nobs, d=d, y=y.astype(int), X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run HMC with Stan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "control = dict(stepsize=1e-2, int_time=1e0, adapt_engaged=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n",
      "WARNING:pystan:Skipping check of divergent transitions (divergence)\n",
      "WARNING:pystan:Skipping check of transitions ending prematurely due to maximum tree depth limit (treedepth)\n",
      "WARNING:pystan:Chain 2: E-BFMI = 0.186\n",
      "WARNING:pystan:E-BFMI below 0.2 indicates you may need to reparameterize your model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37 mins to run\n",
      "Inference for Stan model: anon_model_bf1f70352a45ebcc131d582eff912e1a.\n",
      "4 chains, each with iter=2500; warmup=0; thin=1; \n",
      "post-warmup draws per chain=2500, total post-warmup draws=10000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "xi[1]       -1.74    0.05   0.41  -2.68  -1.98   -1.7  -1.45  -1.07     70   1.06\n",
      "xi[2]       -0.12    0.01   0.63  -1.71  -0.29  -0.03   0.13   0.99   2114    1.0\n",
      "xi[3]       -0.16    0.02   0.59  -1.71   -0.3  -0.04    0.1   0.76   1183    1.0\n",
      "xi[4]        0.03  8.0e-3   0.34  -0.64  -0.11   0.01   0.15   0.82   1783    1.0\n",
      "xi[5]         0.1    0.01   0.43  -0.68  -0.09   0.03   0.22   1.21   1041    1.0\n",
      "xi[6]       -0.65    0.09   1.03  -3.52   -1.0  -0.26-3.7e-3    0.4    120   1.04\n",
      "xi[7]        0.44    0.04   0.65  -0.44 3.0e-3   0.22   0.74   2.15    225   1.02\n",
      "xi[8]       -0.22    0.02   0.56  -1.73  -0.36  -0.07   0.05    0.6    711   1.01\n",
      "xi[9]       -0.11    0.01   0.39  -1.12  -0.26  -0.04   0.07   0.58   1298    1.0\n",
      "xi[10]       0.13    0.01   0.44  -0.61  -0.07   0.05   0.29   1.23   1089   1.01\n",
      "xi[11]      -0.09    0.02   0.61  -1.62  -0.25  -0.02   0.13   1.11   1298    1.0\n",
      "xi[12]       -0.5    0.04   0.78  -2.57   -0.8  -0.23-5.9e-4   0.39    350   1.02\n",
      "xi[13]       -0.3    0.04   0.69  -2.27  -0.45  -0.09   0.04    0.6    243   1.02\n",
      "xi[14]       0.05  7.9e-3    0.4  -0.79  -0.12   0.02   0.21   0.94   2554    1.0\n",
      "xi[15]      -0.11    0.02   0.65   -1.8  -0.25  -0.02   0.13   1.05    808   1.01\n",
      "xi[16]       0.54    0.05   0.92   -0.5-7.2e-3   0.21   0.87   2.91    340   1.02\n",
      "xi[17]     -10.05    0.64   4.19 -20.78  -12.3  -9.37  -7.05  -3.88     42    1.1\n",
      "xi[18]       0.58    0.05   0.92   -0.4-3.8e-3   0.23   0.94   3.07    291   1.01\n",
      "xi[19]       2.19    0.22   2.36  -0.24   0.17   1.46   3.64    7.9    118   1.04\n",
      "xi[20]       0.31    0.03   0.72  -0.62  -0.06   0.09   0.47   2.36    623    1.0\n",
      "xi[21]       0.18    0.02   0.49  -0.58  -0.06   0.06   0.33   1.46    585   1.01\n",
      "xi[22]       0.08    0.01   0.47  -0.83  -0.11   0.03   0.22   1.23   1808    1.0\n",
      "xi[23]        1.0    0.14   1.25   -0.3   0.06   0.55   1.65   4.29     85   1.04\n",
      "xi[24]       0.75    0.07    1.0  -0.33   0.03   0.37   1.23   3.32    234   1.02\n",
      "xi[25]       1.14    0.18   1.68  -0.39   0.03   0.43   1.69   5.99     86   1.05\n",
      "xi[26]       -0.1    0.01   0.47  -1.31  -0.24  -0.03    0.1   0.77   1523    1.0\n",
      "xi[27]        0.2    0.02   0.56  -0.59  -0.07   0.06   0.34   1.74    979    1.0\n",
      "xi[28]      -0.35    0.03   0.69   -2.2  -0.58  -0.14   0.03   0.56    706   1.01\n",
      "xi[29]       0.02    0.02   0.69  -1.49  -0.18 3.1e-3    0.2   1.68   1322    1.0\n",
      "xi[30]    -7.6e-3    0.03   0.73  -1.73  -0.19 2.3e-4   0.19    1.5    607    1.0\n",
      "xi[31]       0.01  9.1e-3   0.47   -1.0  -0.17 3.5e-3   0.18   1.06   2637    1.0\n",
      "xi[32]      -0.65    0.05   0.91  -3.05  -1.06   -0.3  -0.03   0.34    349   1.01\n",
      "xi[33]      -1.27    0.16   1.51   -5.2  -2.05  -0.74  -0.09   0.28     91   1.06\n",
      "xi[34]       0.68    0.08   1.29  -0.59  -0.02   0.18   0.88   4.54    264   1.01\n",
      "xi[35]       0.08    0.01   0.47  -0.83  -0.12   0.03   0.24   1.22   2148    1.0\n",
      "xi[36]       0.04  8.1e-3   0.43  -0.85  -0.12   0.01   0.18   1.06   2773    1.0\n",
      "xi[37]      -0.41    0.08   0.74  -2.48  -0.62  -0.15   0.01   0.44     90   1.06\n",
      "xi[38]      -0.34    0.05   0.89  -2.85  -0.46  -0.07   0.07   0.72    360   1.01\n",
      "xi[39]      -0.03    0.01   0.57  -1.32  -0.21-9.1e-3   0.17   1.22   1936    1.0\n",
      "xi[40]       0.55    0.03   0.73  -0.33   0.02   0.31   0.93   2.37    446   1.01\n",
      "xi[41]       0.05  8.4e-3   0.42  -0.82  -0.12   0.01    0.2   1.06   2556    1.0\n",
      "xi[42]       0.03    0.01    0.5  -1.04  -0.15 4.8e-3   0.19   1.15   1462    1.0\n",
      "xi[43]       0.12    0.01    0.5  -0.81  -0.09   0.03   0.26   1.38   1252   1.01\n",
      "xi[44]      -0.25    0.02   0.57  -1.83   -0.4  -0.08   0.05   0.54    591   1.01\n",
      "xi[45]       0.05  7.2e-3   0.38  -0.69  -0.11   0.01   0.18   0.98   2843    1.0\n",
      "xi[46]       0.32    0.03    0.6  -0.42  -0.02   0.13   0.51   1.99    500   1.01\n",
      "xi[47]      -0.63    0.05   0.94  -3.16  -0.98  -0.28  -0.01   0.37    372   1.02\n",
      "xi[48]       0.08    0.02   0.48  -0.78  -0.12   0.02   0.22    1.3    932    1.0\n",
      "xi[49]       0.12    0.02   0.61  -0.96  -0.13   0.03   0.29   1.73   1033   1.01\n",
      "xi[50]       0.08  9.5e-3   0.45  -0.76  -0.11   0.02   0.22    1.2   2209    1.0\n",
      "xi[51]        0.2    0.02    0.6  -0.66  -0.07   0.05   0.34   1.87   1148    1.0\n",
      "xi[52]       0.35    0.04   0.79  -0.68  -0.05   0.11   0.54    2.5    499   1.01\n",
      "xi[53]       0.28    0.03   0.58   -0.5  -0.03   0.11   0.47   1.81    531   1.01\n",
      "xi[54]      -0.02  9.3e-3   0.48   -1.1  -0.18-2.3e-3   0.16   0.98   2708    1.0\n",
      "xi[55]      -1.15    0.17   1.62  -5.49  -1.75  -0.49  -0.05   0.31     96   1.05\n",
      "xi[56]       0.22    0.02   0.54  -0.57  -0.05   0.08   0.38   1.71    786   1.01\n",
      "xi[57]       0.04    0.01   0.49  -0.97  -0.15 8.7e-3   0.18   1.21   1877    1.0\n",
      "xi[58]     9.5e-3  9.3e-3   0.45  -0.95  -0.16-2.4e-3   0.16   1.06   2340    1.0\n",
      "xi[59]      -0.03  6.5e-3   0.39  -0.95  -0.17  -0.01   0.12   0.76   3577    1.0\n",
      "xi[60]      10.61     0.6   3.77    4.7   7.99  10.08  12.64  19.72     39    1.1\n",
      "xi[61]       0.49    0.05   0.89  -0.47  -0.01   0.15   0.73   3.09    306   1.02\n",
      "xi[62]       0.13    0.01   0.57  -0.89   -0.1   0.03   0.28   1.64   1804    1.0\n",
      "xi[63]       0.13    0.02   0.51  -0.76  -0.09   0.04   0.26   1.47    829   1.01\n",
      "xi[64]       0.19    0.05   0.75  -0.87   -0.1   0.04   0.29   2.19    254   1.02\n",
      "xi[65]       1.98    0.27   2.48   -0.3   0.13   1.05    3.1   8.91     87   1.02\n",
      "xi[66]       0.28    0.03   0.56  -0.42  -0.02   0.12   0.44   1.74    313   1.01\n",
      "xi[67]      -0.15    0.02   0.59  -1.73  -0.29  -0.04    0.1   0.81   1124   1.01\n",
      "xi[68]      -0.08    0.01   0.51  -1.31  -0.22  -0.02   0.11   0.87   1467    1.0\n",
      "xi[69]      -1.93    0.08   1.05  -4.13   -2.6  -1.88  -1.18  -0.06    189   1.02\n",
      "xi[70]       0.11    0.01   0.39  -0.57  -0.07   0.04   0.24   1.12    941    1.0\n",
      "xi[71]       0.14    0.01   0.53  -0.78   -0.1   0.04   0.29   1.54   1404    1.0\n",
      "xi[72]      -8.66    0.47   3.02 -15.94 -10.36  -8.23  -6.51  -3.84     42   1.09\n",
      "xi[73]      -0.14    0.01   0.49  -1.48  -0.27  -0.04   0.08   0.64   1170    1.0\n",
      "xi[74]       0.12    0.01    0.5  -0.75  -0.12   0.04   0.28   1.39   1326    1.0\n",
      "xi[75]       6.64    0.46    3.2   2.27   4.47   6.03   8.11   15.7     48   1.08\n",
      "xi[76]       0.29    0.03   0.58  -0.45  -0.03    0.1   0.47   1.88    400   1.01\n",
      "xi[77]      -0.11    0.02   0.43  -1.26  -0.25  -0.04   0.08   0.68    563   1.01\n",
      "xi[78]       0.06  8.6e-3   0.42  -0.78  -0.11   0.02   0.21   1.04   2369    1.0\n",
      "xi[79]      -0.25    0.02    0.6  -1.91  -0.41  -0.08   0.05   0.62    796   1.01\n",
      "xi[80]       0.21    0.03   0.54   -0.6  -0.06   0.07   0.36   1.63    418   1.01\n",
      "xi[81]       0.44    0.05   0.85  -0.61  -0.03   0.16   0.67   2.86    340   1.01\n",
      "xi[82]      12.22    1.34   7.48   2.65   7.03  10.43  15.66  30.29     31   1.17\n",
      "xi[83]       0.47    0.05   0.84  -0.44  -0.01   0.17   0.69   2.76    290   1.02\n",
      "xi[84]       0.45    0.05   0.87  -0.53  -0.03   0.14   0.68   3.03    356   1.01\n",
      "xi[85]       0.54    0.05   0.81  -0.39 4.9e-3   0.25   0.88   2.58    278   1.03\n",
      "xi[86]      -3.54    0.15    1.9  -7.66  -4.69  -3.49   -2.3  -0.04    157   1.03\n",
      "xi[87]      -0.04    0.02   0.61  -1.45  -0.23-8.6e-3   0.17    1.2   1009    1.0\n",
      "xi[88]      -0.13    0.02   0.46  -1.29  -0.28  -0.04   0.08   0.68    577   1.01\n",
      "xi[89]       0.35    0.03   0.66  -0.53  -0.03   0.15   0.58   2.11    681   1.01\n",
      "xi[90]       0.25    0.03   0.66  -0.71  -0.06   0.07    0.4   2.13    509    1.0\n",
      "xi[91]      -0.07    0.02   0.69  -1.72  -0.23-10.0e-3   0.17   1.25   1109    1.0\n",
      "xi[92]       0.04  9.2e-3   0.38  -0.72  -0.11   0.01   0.17   0.93   1673    1.0\n",
      "xi[93]      -4.35    0.29   2.77 -10.81  -5.79  -3.84  -2.39   -0.3     90   1.03\n",
      "xi[94]      -0.04    0.01   0.44  -1.12  -0.17-5.2e-3   0.14   0.82   1717    1.0\n",
      "xi[95]       0.18    0.02    0.6  -0.77  -0.08   0.05   0.33   1.73    639   1.01\n",
      "xi[96]      -0.02    0.01   0.47  -1.13  -0.17-3.3e-3   0.14   0.95   1731    1.0\n",
      "xi[97]      -0.12    0.02   0.56   -1.6  -0.26  -0.03    0.1   0.86    855    1.0\n",
      "xi[98]        0.3    0.04   0.75  -0.56  -0.04   0.08   0.42   2.34    303   1.03\n",
      "xi[99]      -0.07    0.01   0.45  -1.11  -0.22  -0.02   0.11   0.78   1532    1.0\n",
      "xi[100]       0.1    0.02   0.57  -0.93  -0.11   0.03   0.25   1.46   1137    1.0\n",
      "lambda[1]    1.11    0.04    1.0   0.07   0.39   0.81   1.54   3.73    559   1.01\n",
      "lambda[2]    1.13    0.03   0.97   0.09   0.45   0.86   1.52   3.78    841   1.01\n",
      "lambda[3]    1.29    0.04   1.06   0.12   0.53   1.01   1.74   4.13    906    1.0\n",
      "lambda[4]     1.2    0.04   1.02   0.11   0.47   0.91   1.63   3.85    590    1.0\n",
      "lambda[5]    0.93    0.05   0.95   0.05   0.27    0.6   1.25   3.59    382   1.02\n",
      "lambda[6]    0.98    0.04   0.97   0.07   0.34   0.67   1.29   3.64    527   1.01\n",
      "lambda[7]    1.13    0.04   1.01   0.07   0.41   0.84   1.54   3.77    536   1.01\n",
      "lambda[8]    1.21    0.04   1.01   0.11    0.5   0.93   1.64   3.87    712   1.01\n",
      "lambda[9]    1.15    0.04   0.99    0.1   0.45   0.88   1.54   3.77    684    1.0\n",
      "lambda[10]   1.15    0.04   1.02   0.09   0.42   0.86   1.54   3.92    806    1.0\n",
      "lambda[11]    1.0    0.05   0.96   0.07   0.33   0.68   1.35    3.6    455   1.01\n",
      "lambda[12]   1.12    0.04    1.0   0.08    0.4   0.82   1.53   3.79    663   1.01\n",
      "lambda[13]   1.19    0.04   1.02   0.11   0.47   0.89    1.6   3.93    734    1.0\n",
      "lambda[14]   1.14    0.04   1.03   0.06   0.41   0.85   1.55   3.89    608    1.0\n",
      "lambda[15]   0.96    0.04   0.98   0.05   0.28   0.62    1.3   3.78    502    1.0\n",
      "lambda[16]   0.06  3.1e-3   0.07 5.0e-3   0.02   0.04   0.08    0.2    561   1.01\n",
      "lambda[17]   0.97    0.05   0.97   0.05   0.29   0.66   1.35   3.64    434    1.0\n",
      "lambda[18]    0.6    0.05   0.85   0.02    0.1   0.26   0.73   3.04    281   1.02\n",
      "lambda[19]   1.06    0.04   0.99   0.06   0.36   0.77   1.42   3.72    573    1.0\n",
      "lambda[20]   1.17    0.04   1.03   0.09   0.43   0.86   1.59   3.92    616   1.01\n",
      "lambda[21]    1.2    0.04   1.01    0.1   0.48   0.91   1.59   3.83    774    1.0\n",
      "lambda[22]   0.79    0.04    0.9   0.04   0.19   0.47   1.05   3.26    414   1.01\n",
      "lambda[23]   0.86    0.05    0.9   0.04   0.24   0.55   1.19   3.28    338   1.01\n",
      "lambda[24]   0.79    0.06   0.88   0.04   0.19   0.48   1.07   3.29    244   1.01\n",
      "lambda[25]   1.17    0.04   1.02   0.09   0.44   0.88   1.59   3.89    624    1.0\n",
      "lambda[26]   1.15    0.04   1.02   0.09   0.43   0.85   1.56   3.92    618   1.01\n",
      "lambda[27]   1.03    0.03   0.96   0.06   0.35   0.75    1.4   3.58    828   1.01\n",
      "lambda[28]   1.13    0.05   1.01   0.08   0.41   0.83   1.53   3.82    413   1.02\n",
      "lambda[29]   1.12    0.04    1.0   0.09   0.41   0.81   1.52   3.73    641    1.0\n",
      "lambda[30]   1.18    0.03   1.01    0.1   0.46    0.9   1.58    3.9    903    1.0\n",
      "lambda[31]   0.92    0.04   0.92   0.06   0.27    0.6   1.25   3.42    521    1.0\n",
      "lambda[32]   0.69    0.04   0.83   0.03   0.17   0.39   0.89   3.07    393   1.01\n",
      "lambda[33]   0.94    0.04   0.92   0.04   0.28   0.65   1.29   3.38    513    1.0\n",
      "lambda[34]   1.13    0.04   0.99   0.08   0.42   0.85   1.53   3.82    791    1.0\n",
      "lambda[35]   1.23    0.04   1.04   0.12   0.49   0.94   1.67   4.06    759    1.0\n",
      "lambda[36]   1.03    0.05   0.98   0.07   0.34   0.74    1.4   3.68    473   1.01\n",
      "lambda[37]   1.09    0.05   1.01   0.08   0.38   0.79   1.49   3.77    501    1.0\n",
      "lambda[38]   1.13    0.04   1.03   0.08   0.41   0.81   1.54   3.84    637    1.0\n",
      "lambda[39]   0.92    0.04   0.94   0.05   0.28   0.61   1.23   3.54    516   1.01\n",
      "lambda[40]   1.21    0.04   1.01   0.09   0.49   0.94   1.62   3.82    754   1.01\n",
      "lambda[41]   1.18    0.04   1.03   0.07   0.43   0.88   1.63   3.95    607   1.01\n",
      "lambda[42]    1.2    0.04   1.09   0.08   0.43   0.87   1.63   4.12    618    1.0\n",
      "lambda[43]   1.15    0.04   1.05   0.09   0.41   0.84   1.56   4.04    565   1.01\n",
      "lambda[44]   1.27    0.03   1.03   0.14   0.53   0.99    1.7   4.03    991    1.0\n",
      "lambda[45]   1.13    0.04   1.04   0.09   0.41   0.82   1.51   3.91    707    1.0\n",
      "lambda[46]   0.93    0.04   0.94   0.05   0.28   0.63   1.27   3.51    506   1.01\n",
      "lambda[47]   1.17    0.04   1.01   0.09   0.43   0.88   1.59   3.92    637   1.01\n",
      "lambda[48]   1.08    0.04   0.99   0.07   0.39    0.8   1.45   3.76    643    1.0\n",
      "lambda[49]    1.2    0.04   1.06   0.09   0.45   0.89   1.65   4.07    751    1.0\n",
      "lambda[50]   1.15    0.04   1.06   0.08   0.38   0.83   1.57   3.93    573   1.01\n",
      "lambda[51]   1.03    0.05    1.0   0.07   0.34   0.71   1.38   3.66    407   1.01\n",
      "lambda[52]    1.1    0.04   0.98   0.08   0.41   0.81   1.46   3.75    677    1.0\n",
      "lambda[53]   1.15    0.04    1.0   0.09   0.45   0.86   1.54   3.79    753    1.0\n",
      "lambda[54]   0.78    0.04   0.91   0.03   0.19   0.45   1.04   3.35    438   1.01\n",
      "lambda[55]   1.16    0.05   1.07   0.07   0.41   0.85   1.55   4.17    485   1.01\n",
      "lambda[56]    1.2    0.04   1.03   0.09   0.45   0.91   1.63   3.86    608   1.01\n",
      "lambda[57]    1.2    0.03   0.99   0.12   0.49   0.92   1.62   3.83    916    1.0\n",
      "lambda[58]   1.25    0.04   1.06   0.11    0.5   0.95    1.7   4.06    764    1.0\n",
      "lambda[59]   0.05  2.1e-3   0.06 4.8e-3   0.02   0.04   0.07   0.19    793   1.01\n",
      "lambda[60]   1.04    0.04   1.01   0.05   0.33    0.7   1.41   3.87    571   1.01\n",
      "lambda[61]   1.14    0.04   1.03   0.08   0.41   0.83   1.54   3.86    672   1.01\n",
      "lambda[62]   1.17    0.04   1.03   0.08   0.43   0.88    1.6   3.86    570   1.01\n",
      "lambda[63]   1.13    0.04   1.04   0.09   0.41   0.82   1.53   3.86    607   1.01\n",
      "lambda[64]   0.62    0.05   0.82   0.02   0.11   0.29   0.79   2.95    226   1.01\n",
      "lambda[65]    1.1    0.04   0.99   0.09    0.4   0.81   1.49   3.79    724    1.0\n",
      "lambda[66]   1.13    0.04    1.0   0.09   0.43   0.86   1.51   3.89    680   1.01\n",
      "lambda[67]    1.2    0.04   1.08   0.08   0.43    0.9   1.63   4.02    594   1.01\n",
      "lambda[68]   0.34    0.02   0.45   0.02    0.1   0.21    0.4   1.54    414   1.01\n",
      "lambda[69]   1.23    0.04   1.04   0.13    0.5   0.93   1.64   3.98    717    1.0\n",
      "lambda[70]   1.13    0.04   1.02   0.08   0.41   0.82   1.52   3.86    581   1.01\n",
      "lambda[71]   0.07  2.7e-3   0.08 6.4e-3   0.03   0.05   0.08   0.22    825   1.01\n",
      "lambda[72]    1.2    0.04   1.03   0.12   0.47    0.9   1.62   3.96    811   1.01\n",
      "lambda[73]   1.11    0.04    1.0   0.09   0.42   0.81   1.49   3.88    720   1.01\n",
      "lambda[74]   0.09  3.7e-3    0.1 8.4e-3   0.03   0.06   0.11   0.32    739    1.0\n",
      "lambda[75]   1.15    0.04   1.02   0.09   0.41   0.84   1.56   3.88    538    1.0\n",
      "lambda[76]   1.17    0.04   1.02    0.1   0.44   0.87   1.55   3.95    839    1.0\n",
      "lambda[77]   1.19    0.04   1.02   0.09   0.47    0.9   1.62   3.87    674    1.0\n",
      "lambda[78]   1.11    0.04   0.99   0.08   0.39   0.82   1.53   3.74    732    1.0\n",
      "lambda[79]   1.13    0.04   0.99    0.1   0.44   0.85   1.52   3.81    707    1.0\n",
      "lambda[80]   0.97    0.04   0.93   0.05   0.31   0.69   1.32    3.5    568    1.0\n",
      "lambda[81]   0.06  4.6e-3    0.1 4.6e-3   0.02   0.04   0.07   0.24    438   1.01\n",
      "lambda[82]   0.99    0.04   0.92   0.06   0.34    0.7   1.34   3.53    437   1.02\n",
      "lambda[83]   1.04    0.05   1.03   0.05   0.33   0.71   1.42   3.88    495    1.0\n",
      "lambda[84]   0.93    0.04   0.93   0.05   0.29   0.64   1.23   3.51    435   1.01\n",
      "lambda[85]   0.23    0.02   0.38   0.02   0.06   0.12   0.23   1.34    388   1.01\n",
      "lambda[86]   1.07    0.04   0.96   0.09   0.39   0.78   1.44   3.57    739   1.01\n",
      "lambda[87]   1.18    0.04   1.03    0.1   0.45   0.89   1.59   3.93    631   1.01\n",
      "lambda[88]   0.99    0.04   0.92   0.06   0.35   0.71   1.32   3.54    621   1.01\n",
      "lambda[89]   1.13    0.04   1.06   0.07   0.38   0.81   1.52   3.99    574    1.0\n",
      "lambda[90]   1.11    0.04   1.03   0.06   0.38    0.8   1.53   3.83    646   1.01\n",
      "lambda[91]   1.21    0.04   1.05   0.08   0.46   0.92   1.65   3.99    787    1.0\n",
      "lambda[92]   0.18    0.02    0.3   0.01   0.05    0.1    0.2   0.89    317   1.01\n",
      "lambda[93]   1.24    0.04   1.06   0.09   0.49   0.95   1.65   4.04    703    1.0\n",
      "lambda[94]   1.11    0.04   0.97   0.09   0.42   0.83   1.48   3.71    715    1.0\n",
      "lambda[95]   1.23    0.03   1.03   0.11    0.5   0.94   1.65   3.94    881    1.0\n",
      "lambda[96]   1.17    0.04   1.04   0.07   0.42   0.87   1.61   3.85    578    1.0\n",
      "lambda[97]   1.13    0.04   1.04   0.07   0.38   0.81   1.55   3.92    545    1.0\n",
      "lambda[98]   1.21    0.04    1.0   0.12    0.5   0.93   1.65   3.81    776    1.0\n",
      "lambda[99]   1.16    0.04   1.01    0.1   0.44   0.87   1.57   3.94    825    1.0\n",
      "tau[1]     120.04   85.73 2406.1   0.03   0.42   1.91   8.86 249.63    788    1.0\n",
      "tau[2]      19.62    3.31  94.87   0.04   0.44   1.62   6.14 163.73    819    1.0\n",
      "tau[3]      25.25   13.43 395.47   0.02   0.28   1.05   3.97  106.7    867   1.01\n",
      "tau[4]      14.18    2.47   77.8   0.02   0.36   1.36   5.86  99.95    990    1.0\n",
      "tau[5]     1401.1  1338.9  3.8e4   0.03    0.7   3.83  21.99 536.94    787    1.0\n",
      "tau[6]      28.74     4.3 151.04   0.03   0.66   3.15  12.88  221.8   1234    1.0\n",
      "tau[7]      50.54   16.94 477.46   0.02    0.4   1.69   8.02 281.51    794   1.01\n",
      "tau[8]      16.27    4.83  165.3   0.02   0.35   1.28   5.05   97.8   1172    1.0\n",
      "tau[9]      21.54    4.88 169.11   0.03   0.38   1.64   6.24 129.12   1202    1.0\n",
      "tau[10]     23.41    6.99  237.8   0.03    0.4    1.6   7.52 141.95   1158    1.0\n",
      "tau[11]      29.8    4.33 145.54   0.03   0.61   2.83  13.93 216.04   1129    1.0\n",
      "tau[12]      22.1    4.67 175.23   0.02   0.42   1.88   8.46 166.52   1408    1.0\n",
      "tau[13]     15.05    4.05 178.25   0.02   0.38   1.41   5.56  78.79   1941    1.0\n",
      "tau[14]     56.77   18.04 431.26   0.02   0.39   1.66    8.2 319.39    571   1.01\n",
      "tau[15]     123.4   43.42 1406.4   0.03   0.62    3.4  21.04 589.36   1049    1.0\n",
      "tau[16]    8794.4  1691.3  5.9e4   83.6 432.01 1145.5 3278.2  5.2e4   1199    1.0\n",
      "tau[17]    142.44   68.87 2406.1   0.03   0.57   3.11  19.27 586.83   1221    1.0\n",
      "tau[18]    1803.2  1267.5  4.8e4   0.05   2.71  30.64 184.48 3498.1   1429    1.0\n",
      "tau[19]    279.53  207.55 6601.3   0.03   0.49    2.2  10.28 321.29   1012    1.0\n",
      "tau[20]     32.89   11.26 455.25   0.02   0.38   1.62   7.42 141.58   1636    1.0\n",
      "tau[21]     17.09    4.01  140.3   0.02   0.37   1.36   5.45 102.56   1225    1.0\n",
      "tau[22]    149.41   51.11 3070.6   0.04   1.09   7.67  44.03 821.37   3610    1.0\n",
      "tau[23]    111.63   27.17 1027.1   0.04   0.86    4.9  27.22 678.11   1429    1.0\n",
      "tau[24]    131.67    29.7 880.55   0.05    1.1   7.03   45.1 876.26    879   1.01\n",
      "tau[25]     33.92   12.38 413.53   0.02   0.36    1.6   6.77 131.97   1116    1.0\n",
      "tau[26]      23.2    6.66 187.65   0.02   0.37   1.59   7.14 143.86    794    1.0\n",
      "tau[27]     42.39    9.85 229.15   0.04   0.53   2.23  11.03 330.11    541   1.01\n",
      "tau[28]     28.15    5.85 189.06   0.02    0.4   1.85   8.28  209.9   1046    1.0\n",
      "tau[29]     21.58    4.45 121.59   0.02    0.4    1.8   8.31 160.33    746    1.0\n",
      "tau[30]     22.32    5.76 204.75   0.03    0.4    1.5   5.81 113.01   1265    1.0\n",
      "tau[31]     56.19    9.99  326.0   0.04   0.73   3.96  20.73 399.15   1064    1.0\n",
      "tau[32]    239.88    98.5 2432.9   0.06   1.77  12.06  58.09 1113.9    610   1.01\n",
      "tau[33]     96.65    25.2 645.12   0.04   0.66   3.18  19.27 701.01    656    1.0\n",
      "tau[34]      35.3    8.91  335.7   0.03   0.42   1.67   7.19 193.34   1420    1.0\n",
      "tau[35]     10.95     1.8  58.01   0.02   0.31   1.24   5.08  76.98   1037    1.0\n",
      "tau[36]     41.59    9.56 339.55   0.03   0.53   2.42  11.48 262.52   1261    1.0\n",
      "tau[37]     29.68    5.32 200.59   0.03   0.45   2.06   9.77 207.79   1419    1.0\n",
      "tau[38]     32.56    9.61 257.35   0.02   0.41   1.77   8.17  200.9    717   1.01\n",
      "tau[39]    532.35  464.39 9583.0   0.04   0.78   4.07  18.64  649.9    426   1.01\n",
      "tau[40]    119.23   74.05 2139.8   0.03   0.37   1.34   4.89 134.65    835    1.0\n",
      "tau[41]    104.93   67.73 2293.1   0.02   0.35    1.5   7.02 234.33   1146    1.0\n",
      "tau[42]    161.17  122.81 3435.6   0.02   0.33   1.55   7.14 199.05    783   1.01\n",
      "tau[43]      24.1    4.86 187.39   0.02    0.4   1.75   7.68 172.79   1489    1.0\n",
      "tau[44]       9.1    1.36  53.58   0.02   0.31   1.09   4.05  60.81   1551    1.0\n",
      "tau[45]     28.93    7.31 247.43   0.02   0.44    1.8   7.94 170.76   1144   1.01\n",
      "tau[46]     54.72   11.29 339.93   0.04   0.72   3.52  18.99 431.61    906    1.0\n",
      "tau[47]     25.72    8.31  311.9   0.03   0.36   1.56   6.88 151.69   1410    1.0\n",
      "tau[48]     59.86   24.96 981.13   0.03   0.48   1.99   8.21 231.07   1545    1.0\n",
      "tau[49]     32.17   14.62 540.99   0.02   0.32   1.38   6.15 137.86   1368    1.0\n",
      "tau[50]     23.89    5.91 178.54   0.02   0.37   1.74   8.76 162.61    914   1.01\n",
      "tau[51]      61.5    26.5 1067.2   0.03   0.53   2.61  12.43 261.83   1622    1.0\n",
      "tau[52]     31.04    8.72 300.73   0.03   0.46    1.9   7.97 176.69   1190    1.0\n",
      "tau[53]     94.99   63.12 2647.0   0.03   0.38   1.57   6.51 150.95   1759    1.0\n",
      "tau[54]    231.07    78.6 2902.6   0.03   1.16   8.07  48.95 1380.0   1364    1.0\n",
      "tau[55]     148.4  101.31 3401.4   0.02    0.4   1.67    7.5 207.21   1127    1.0\n",
      "tau[56]     21.51    5.51 170.83   0.02   0.35   1.37   5.66 125.13    961   1.01\n",
      "tau[57]     12.92    2.68  90.68   0.02   0.37   1.36   5.27  74.57   1142    1.0\n",
      "tau[58]    434.07  420.15  1.6e4   0.02    0.3   1.22   4.82   81.8   1487    1.0\n",
      "tau[59]    7861.2  1342.7  4.6e4  90.59 529.02 1373.9 4004.9  4.9e4   1162   1.01\n",
      "tau[60]     90.46   30.95 989.31   0.02   0.49   2.54  14.01 481.28   1022    1.0\n",
      "tau[61]     27.37    7.17 240.09   0.02    0.4    1.7   7.62 142.56   1121    1.0\n",
      "tau[62]      44.9   17.83 606.78   0.01   0.37   1.52    6.9 186.43   1158    1.0\n",
      "tau[63]      20.1    3.52 117.93   0.02    0.4   1.79   8.05 135.26   1122    1.0\n",
      "tau[64]    874.49  375.03  1.3e4   0.06   2.14  20.44 153.53 3455.5   1219    1.0\n",
      "tau[65]     20.28    3.18 102.35   0.03   0.46   1.89   8.64  170.6   1035    1.0\n",
      "tau[66]     26.02    9.49 248.13   0.02   0.42   1.63   6.89 156.47    684    1.0\n",
      "tau[67]    126.67   78.13 2608.3   0.02   0.33   1.43   6.94 225.86   1115    1.0\n",
      "tau[68]     565.2   139.7 4605.6   0.52  13.63  46.15 163.17 2763.8   1087    1.0\n",
      "tau[69]      9.57    1.32  49.81   0.02   0.34   1.36   5.17  66.29   1423    1.0\n",
      "tau[70]     33.62   10.31 390.73   0.03   0.39   1.77   8.17 184.98   1437    1.0\n",
      "tau[71]    5224.9  1502.1  5.6e4  64.96 345.58  859.9 2424.1  2.5e4   1402    1.0\n",
      "tau[72]     21.29   10.23  441.1   0.02   0.36   1.39    5.8  92.61   1858    1.0\n",
      "tau[73]      30.1    9.77 308.98   0.03   0.46   1.92   7.79 146.82    999    1.0\n",
      "tau[74]    3722.1  1078.8  3.9e4  30.59 193.43 521.48 1506.4  1.8e4   1332    1.0\n",
      "tau[75]     30.66   11.98 436.35   0.02    0.4   1.73   7.93 139.16   1327    1.0\n",
      "tau[76]     16.27    3.18  171.6   0.02   0.36   1.49    6.4 107.12   2913    1.0\n",
      "tau[77]    139.96  108.03 4090.7   0.03   0.35   1.43   5.93 115.61   1434    1.0\n",
      "tau[78]     31.51    7.26 307.31   0.03   0.41   1.78   9.17 196.87   1791    1.0\n",
      "tau[79]     20.09    4.63 195.83   0.03   0.43   1.76   6.76 111.47   1791    1.0\n",
      "tau[80]    110.65   41.93 1129.7   0.05   0.63   2.91  15.66 491.43    726    1.0\n",
      "tau[81]     2.2e4   1.3e4  2.8e5  52.07 514.81 1556.6 4882.2  6.2e4    474   1.01\n",
      "tau[82]     82.35   38.89 1041.2   0.04   0.62   2.61   12.5 343.96    717   1.01\n",
      "tau[83]      81.3   26.69 879.85   0.02   0.48   2.61  13.69 460.94   1087    1.0\n",
      "tau[84]     76.35   19.72  680.4   0.03   0.74   3.41  17.81 476.21   1191    1.0\n",
      "tau[85]    956.82  213.76 7246.6   0.69  46.78 138.34 405.07 4907.6   1149    1.0\n",
      "tau[86]     26.99    7.38 216.35   0.03   0.51   2.16   9.05 150.12    860    1.0\n",
      "tau[87]     19.81     4.6 190.81   0.02   0.36    1.5   6.39 118.78   1719    1.0\n",
      "tau[88]     81.28   46.79 1827.1   0.04   0.64   2.66  12.15 310.67   1525    1.0\n",
      "tau[89]     55.23   25.21 1327.2   0.01   0.37   1.85   9.87 232.69   2772    1.0\n",
      "tau[90]     87.08    42.6 1704.0   0.02   0.41   1.87   8.87 351.83   1600    1.0\n",
      "tau[91]    254.06   198.4 5933.1   0.02   0.33   1.27   5.52 193.49    894   1.01\n",
      "tau[92]    1525.6  305.12 9626.9   2.26  56.97 185.72 618.33 9347.8    996    1.0\n",
      "tau[93]    442.03  425.48  1.1e4   0.02   0.32   1.21   5.31  159.9    674   1.01\n",
      "tau[94]     90.97   73.78 2879.0   0.03   0.45   1.77   7.32 143.68   1523    1.0\n",
      "tau[95]     28.59   12.15 505.27   0.03   0.33   1.23   4.83  91.21   1730    1.0\n",
      "tau[96]     29.44    5.68 228.08   0.02   0.36   1.54   7.39 227.29   1614    1.0\n",
      "tau[97]     33.93   10.21  236.0   0.02    0.4   1.76    9.2 217.35    534   1.01\n",
      "tau[98]      9.78    1.28  47.43   0.02   0.38   1.38   4.87  67.52   1373    1.0\n",
      "tau[99]     16.91     3.9 108.69   0.02    0.4    1.6   6.64 102.77    775   1.01\n",
      "sigma2       0.09    0.01   0.35 9.7e-3   0.03   0.06   0.11   0.32    910    1.0\n",
      "lp__       -336.5    5.02  34.53 -405.5 -357.3 -335.4 -313.8 -272.5     47   1.11\n",
      "\n",
      "Samples were drawn using HMC at Mon Mar 25 16:20:51 2019.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "fit_gdP = sm_gdP.sampling(data=data, \n",
    "                          thin=1, \n",
    "                          control=control, \n",
    "                          n_jobs=4, \n",
    "                          init=\"random\", \n",
    "                          iter=2500, \n",
    "                          algorithm=\"HMC\", \n",
    "                          warmup=0)\n",
    "print(round((time()-start)/60, 2), \"mins to run\")\n",
    "print(fit_gdP);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean effective sample size: 917.9\n"
     ]
    }
   ],
   "source": [
    "a = fit_gdP.summary()[\"summary\"]\n",
    "ess = a[:,-2]\n",
    "print(\"Mean effective sample size:\", np.round(np.mean(ess),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace = fit_gdP.extract()\n",
    "xi_samples = trace[\"xi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(xi_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cover = np.zeros(d)\n",
    "ci = np.zeros((d,2))\n",
    "for i in range(d) :\n",
    "    ci[i,:] = np.percentile(xi_samples[:,i], q=[2.5, 97.5])\n",
    "    cover[i] = (ci[i,0]<両_true[i])&(両_true[i]<ci[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+IAAAEyCAYAAAB+u2pNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<Figure size 1224x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig= plt.figure(figsize=(17,5))\n",
    "for i in range(d) :\n",
    "    plt.plot([i+1,i+1], ci[i], color=\"blue\", lw=1.5)\n",
    "plt.plot(np.arange(1,d+1), 両_true, \"ro\", markersize=4)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*np.mean(cover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
