{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, pystan as ps, numpy.random as npr, matplotlib.pyplot as plt, h5py\n",
    "%matplotlib inline \n",
    "from time import time\n",
    "from pylab import plot, show, legend\n",
    "from scipy.stats import pearsonr, spearmanr, norm, invgamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile Stan model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_bf1f70352a45ebcc131d582eff912e1a NOW.\n"
     ]
    }
   ],
   "source": [
    "sm_gdP = ps.StanModel(file=\"gdP_logistic.stan\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = h5py.File(\"GZZ_data2.jld\", \"r\")\n",
    "X = data[\"X\"].value\n",
    "y = data[\"y\"].value\n",
    "両_true = data[\"xi_true\"].value\n",
    "d, Nobs = np.shape(X.transpose())\n",
    "\n",
    "data = dict(N=Nobs, d=d, y=y.astype(int), X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run HMC with Stan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "control = dict(stepsize=1e-2, int_time=1e0, adapt_engaged=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n",
      "WARNING:pystan:Skipping check of divergent transitions (divergence)\n",
      "WARNING:pystan:Skipping check of transitions ending prematurely due to maximum tree depth limit (treedepth)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42 mins to run\n",
      "Inference for Stan model: anon_model_bf1f70352a45ebcc131d582eff912e1a.\n",
      "4 chains, each with iter=2500; warmup=0; thin=1; \n",
      "post-warmup draws per chain=2500, total post-warmup draws=10000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "xi[1]       -1.76    0.05   0.42  -2.72  -2.02  -1.71  -1.46  -1.05     71   1.05\n",
      "xi[2]       -0.09    0.02   0.64   -1.7  -0.26  -0.02   0.13   1.17   1262    1.0\n",
      "xi[3]       -0.19    0.02   0.68  -2.11  -0.35  -0.05    0.1   0.88   1137    1.0\n",
      "xi[4]        0.05  6.5e-3   0.35  -0.62  -0.12   0.01   0.18   0.87   2885    1.0\n",
      "xi[5]        0.09  9.2e-3   0.39  -0.59  -0.08   0.03   0.21   1.07   1821   1.01\n",
      "xi[6]       -0.57    0.07   0.94  -3.12  -0.88  -0.22 6.9e-3   0.49    196   1.02\n",
      "xi[7]        0.46    0.04   0.71   -0.4 3.4e-3   0.23   0.75   2.38    283   1.02\n",
      "xi[8]       -0.19    0.02   0.53  -1.63  -0.33  -0.06   0.06   0.62    745    1.0\n",
      "xi[9]        -0.1  9.5e-3   0.38  -1.05  -0.25  -0.04   0.08   0.59   1622    1.0\n",
      "xi[10]       0.11    0.01   0.41  -0.62  -0.08   0.04   0.24   1.14   1269    1.0\n",
      "xi[11]       -0.1    0.02   0.59  -1.64  -0.26  -0.02   0.13    1.0   1187   1.01\n",
      "xi[12]      -0.56    0.04   0.84  -2.78  -0.91  -0.25-5.7e-3   0.41    462   1.01\n",
      "xi[13]      -0.28    0.03   0.66   -2.1  -0.45  -0.09   0.05   0.62    593   1.01\n",
      "xi[14]       0.05  8.2e-3    0.4  -0.81  -0.11   0.02   0.18   0.99   2321    1.0\n",
      "xi[15]      -0.12    0.02   0.61   -1.7  -0.28  -0.03   0.14   0.96   1000   1.01\n",
      "xi[16]       0.58    0.06   0.93  -0.48-2.7e-3   0.25   0.89   3.15    273   1.01\n",
      "xi[17]      -9.67    0.44   3.73 -18.05 -11.88  -9.26  -6.98  -3.47     73   1.05\n",
      "xi[18]       0.71    0.06   1.01  -0.42   0.01   0.31   1.19   3.39    265    1.0\n",
      "xi[19]       2.65     0.3   2.64  -0.21   0.33   1.98   4.31   8.92     77   1.07\n",
      "xi[20]       0.25    0.03   0.68  -0.66  -0.07   0.06   0.38   2.19    735   1.01\n",
      "xi[21]       0.17    0.01   0.47  -0.57  -0.06   0.06   0.32   1.37    991    1.0\n",
      "xi[22]       0.07  9.5e-3   0.47  -0.86  -0.11   0.02   0.22    1.2   2512    1.0\n",
      "xi[23]       0.99    0.11   1.25  -0.37   0.06   0.52   1.66   4.25    129   1.03\n",
      "xi[24]       0.68    0.08   0.97  -0.38   0.02   0.32   1.09   3.28    164   1.02\n",
      "xi[25]        1.0     0.1   1.44  -0.46   0.02   0.42   1.61   4.86    218   1.02\n",
      "xi[26]      -0.07  6.8e-3   0.44  -1.15  -0.22  -0.02   0.11   0.77   4207    1.0\n",
      "xi[27]       0.18    0.01   0.53  -0.66  -0.07   0.05   0.32   1.62   1289    1.0\n",
      "xi[28]      -0.32    0.03   0.66  -2.22  -0.51  -0.11   0.04   0.54    516   1.01\n",
      "xi[29]      -0.07    0.02   0.69  -1.84  -0.22  -0.01   0.17   1.17   1336   1.01\n",
      "xi[30]    -8.1e-3    0.02   0.72  -1.68   -0.2-1.1e-4   0.21   1.57   1436    1.0\n",
      "xi[31]       0.01  8.1e-3   0.46  -0.99  -0.17 3.2e-3   0.18   1.05   3304    1.0\n",
      "xi[32]      -0.67    0.05   0.88  -2.89  -1.12  -0.35  -0.03   0.34    313   1.02\n",
      "xi[33]       -1.1    0.13   1.37   -4.7  -1.79  -0.57  -0.07   0.26    106   1.02\n",
      "xi[34]       0.81    0.11   1.51  -0.58  -0.01   0.21   1.07   5.47    206   1.04\n",
      "xi[35]       0.08  9.2e-3   0.46  -0.76  -0.12   0.02   0.22   1.26   2474    1.0\n",
      "xi[36]       0.04  8.8e-3   0.43  -0.81  -0.13 8.3e-3   0.17   1.05   2372    1.0\n",
      "xi[37]      -0.34    0.03   0.64  -2.09  -0.55  -0.13   0.03   0.45    399   1.01\n",
      "xi[38]      -0.37    0.05   0.97  -3.14  -0.48  -0.09   0.06   0.73    385   1.01\n",
      "xi[39]      -0.02    0.01   0.54  -1.24  -0.19-8.7e-3   0.15   1.14   1891    1.0\n",
      "xi[40]       0.56    0.04   0.75  -0.34   0.02   0.31   0.95    2.5    358   1.02\n",
      "xi[41]       0.06    0.01   0.46  -0.79  -0.12   0.02    0.2    1.2   1244   1.01\n",
      "xi[42]       0.04    0.01   0.49  -0.93  -0.15 5.8e-3   0.19   1.21   1398   1.01\n",
      "xi[43]       0.12    0.01    0.5  -0.76  -0.09   0.04   0.29   1.36   1743    1.0\n",
      "xi[44]      -0.22    0.02   0.56  -1.72  -0.37  -0.07   0.05   0.59    660   1.01\n",
      "xi[45]       0.05  7.6e-3    0.4  -0.75  -0.11   0.02    0.2   0.99   2790    1.0\n",
      "xi[46]        0.4    0.04   0.67  -0.42-6.5e-3   0.18   0.65   2.22    265   1.02\n",
      "xi[47]      -0.67    0.06   1.02  -3.46  -1.06  -0.27-8.0e-3    0.4    289   1.01\n",
      "xi[48]       0.06    0.01   0.47  -0.83  -0.12   0.01    0.2   1.26   1847    1.0\n",
      "xi[49]       0.12    0.02    0.6  -1.02  -0.12   0.03   0.28   1.59   1513    1.0\n",
      "xi[50]       0.09    0.01   0.45  -0.69  -0.11   0.02   0.23   1.23   1835    1.0\n",
      "xi[51]       0.17    0.02   0.56  -0.68  -0.08   0.04    0.3   1.65   1164    1.0\n",
      "xi[52]        0.4    0.05    0.9  -0.64  -0.04    0.1   0.54   2.86    363   1.03\n",
      "xi[53]       0.29    0.03   0.63  -0.55  -0.03    0.1   0.46   2.08    448   1.01\n",
      "xi[54]      -0.02  8.1e-3   0.44  -1.05  -0.16-5.3e-3   0.15   0.91   2954    1.0\n",
      "xi[55]      -1.34     0.2   1.78  -6.11  -2.11  -0.66  -0.07   0.36     78   1.07\n",
      "xi[56]       0.26    0.02   0.58  -0.59  -0.04   0.09   0.45   1.82    805   1.01\n",
      "xi[57]       0.03  9.1e-3   0.49  -0.96  -0.16 5.8e-3   0.19   1.21   2845    1.0\n",
      "xi[58]       0.02    0.01   0.45   -0.9  -0.13 2.4e-3   0.16   1.07   1820    1.0\n",
      "xi[59]      -0.04  8.6e-3   0.41  -0.97   -0.2  -0.01   0.12   0.79   2305    1.0\n",
      "xi[60]      10.72     0.5   4.14   4.54   7.79  10.07  12.82  20.45     69   1.06\n",
      "xi[61]       0.46    0.05   0.86  -0.49  -0.02   0.15   0.65   2.95    351   1.01\n",
      "xi[62]       0.13    0.02   0.56  -0.85  -0.09   0.03   0.27    1.6   1043   1.01\n",
      "xi[63]       0.07    0.01   0.45  -0.77  -0.11   0.03   0.22   1.13   1994    1.0\n",
      "xi[64]       0.14    0.02   0.59  -0.83  -0.12   0.04   0.32   1.59   1201   1.01\n",
      "xi[65]        2.2    0.27   2.42  -0.28   0.16   1.49   3.61   8.11     82   1.08\n",
      "xi[66]       0.31    0.02   0.57  -0.44  -0.02   0.13    0.5   1.87    669   1.01\n",
      "xi[67]      -0.19    0.02   0.64  -1.87  -0.36  -0.05   0.09    0.8    917   1.01\n",
      "xi[68]      -0.07    0.01    0.5  -1.29  -0.23  -0.02   0.12    0.9   1926    1.0\n",
      "xi[69]      -1.95     0.1   1.06  -4.23  -2.62  -1.91  -1.22  -0.05    109   1.04\n",
      "xi[70]        0.1    0.01    0.4   -0.6  -0.08   0.04   0.24   1.13   1468    1.0\n",
      "xi[71]       0.14    0.01   0.51  -0.78   -0.1   0.05    0.3   1.48   1699    1.0\n",
      "xi[72]      -8.89     0.5   3.36 -16.37 -10.93  -8.33  -6.39   -3.7     45    1.1\n",
      "xi[73]      -0.16    0.02   0.51  -1.45  -0.29  -0.05   0.07   0.59   1048   1.01\n",
      "xi[74]       0.15    0.02   0.51  -0.68  -0.09   0.04   0.29   1.59    980    1.0\n",
      "xi[75]       6.75    0.36   3.21   2.33   4.56   6.11   8.26   14.8     78   1.05\n",
      "xi[76]       0.25    0.02   0.53  -0.44  -0.04    0.1   0.42    1.7    733   1.01\n",
      "xi[77]      -0.11  9.6e-3   0.41  -1.13  -0.26  -0.04   0.08    0.6   1802   1.01\n",
      "xi[78]       0.07  7.4e-3   0.44  -0.82  -0.12   0.03   0.23   1.13   3480    1.0\n",
      "xi[79]      -0.23    0.02   0.59  -1.82  -0.38  -0.07   0.06    0.6    841   1.01\n",
      "xi[80]       0.18    0.01    0.5  -0.62  -0.06   0.07   0.35   1.49   1144    1.0\n",
      "xi[81]       0.29    0.03   0.65  -0.58  -0.04   0.09   0.45   2.08    643   1.01\n",
      "xi[82]      12.15    1.08   6.32   3.46   7.39  10.81  15.48  27.42     34   1.11\n",
      "xi[83]       0.53    0.06   0.91  -0.52  -0.01    0.2   0.81   3.06    221   1.03\n",
      "xi[84]       0.45    0.06   0.87  -0.51  -0.03   0.12   0.66   2.93    217   1.02\n",
      "xi[85]       0.49    0.03   0.75  -0.38 2.7e-3   0.23   0.82   2.48    544   1.01\n",
      "xi[86]       -3.6    0.21    2.1  -8.32  -4.78  -3.43  -2.18  -0.07    101   1.03\n",
      "xi[87]      -0.09    0.02   0.57  -1.51  -0.25  -0.02   0.14   0.96    905    1.0\n",
      "xi[88]      -0.12    0.01   0.44   -1.2  -0.28  -0.05   0.09    0.7   1356    1.0\n",
      "xi[89]       0.34    0.03   0.65  -0.54  -0.03   0.12   0.56   2.09    590    1.0\n",
      "xi[90]       0.29    0.03   0.69  -0.66  -0.06   0.09   0.48   2.19    593   1.01\n",
      "xi[91]      -0.12    0.03   0.81  -2.16  -0.28  -0.02   0.16   1.37    863    1.0\n",
      "xi[92]       0.04  9.7e-3    0.4  -0.77  -0.13   0.01   0.19   0.98   1673    1.0\n",
      "xi[93]      -4.23    0.27   2.72 -11.43  -5.45  -3.67   -2.4  -0.31     99   1.02\n",
      "xi[94]      -0.05    0.01   0.46  -1.13   -0.2  -0.01   0.14   0.83   1806    1.0\n",
      "xi[95]        0.2    0.02   0.62  -0.76  -0.09   0.05   0.35   1.93    661    1.0\n",
      "xi[96]    -7.6e-3    0.01   0.51  -1.14  -0.17-1.3e-3   0.17   1.13   2401    1.0\n",
      "xi[97]       -0.1    0.01   0.55  -1.47  -0.24  -0.03   0.11   0.94   1464    1.0\n",
      "xi[98]       0.25    0.03   0.62  -0.54  -0.05   0.07   0.39   2.01    447   1.01\n",
      "xi[99]      -0.09    0.01    0.5  -1.32  -0.26  -0.03   0.12   0.84   1655   1.01\n",
      "xi[100]       0.1    0.01   0.54  -0.89  -0.11   0.02   0.24   1.53   1545    1.0\n",
      "lambda[1]    1.13    0.04   1.02   0.08    0.4   0.83   1.53   3.85    614   1.01\n",
      "lambda[2]    1.07    0.04   0.98   0.07   0.37   0.77   1.43   3.78    622    1.0\n",
      "lambda[3]    1.22    0.04   1.02   0.12   0.49   0.94   1.66    3.9    835   1.01\n",
      "lambda[4]    1.29    0.04   1.05   0.11   0.53   1.01   1.76   4.08    870    1.0\n",
      "lambda[5]    0.95    0.04   0.96   0.05    0.3   0.63   1.27   3.72    495    1.0\n",
      "lambda[6]    0.97    0.04   0.97   0.06   0.31   0.66   1.31   3.61    520   1.01\n",
      "lambda[7]    1.16    0.04   1.01    0.1   0.45   0.87   1.57    3.9    760   1.01\n",
      "lambda[8]    1.21    0.04   1.02   0.11   0.49   0.93   1.64   3.97    656   1.01\n",
      "lambda[9]    1.23    0.04   1.06   0.09   0.48   0.94   1.68   4.14    618   1.01\n",
      "lambda[10]   1.17    0.04   1.05    0.1   0.42   0.86   1.61   3.91    625    1.0\n",
      "lambda[11]   0.94    0.04   0.95   0.06   0.29   0.62   1.27   3.58    465    1.0\n",
      "lambda[12]   1.11    0.04   1.01   0.08    0.4   0.79    1.5    3.8    688    1.0\n",
      "lambda[13]   1.25    0.04   1.05    0.1   0.48   0.97    1.7   4.03    761   1.01\n",
      "lambda[14]   1.09    0.04   1.01   0.09   0.39   0.78   1.48   3.78    619    1.0\n",
      "lambda[15]   0.92    0.04    0.9   0.06    0.3   0.64   1.24   3.41    656    1.0\n",
      "lambda[16]   0.06  3.6e-3   0.09 5.8e-3   0.02   0.04   0.08   0.22    586   1.01\n",
      "lambda[17]   0.89    0.05   0.93   0.05   0.24   0.58   1.22   3.47    417   1.01\n",
      "lambda[18]   0.48    0.05    0.7   0.02   0.09    0.2   0.56   2.49    211   1.03\n",
      "lambda[19]   1.16    0.04   1.06   0.08   0.41   0.85   1.58    4.0    596   1.01\n",
      "lambda[20]   1.19    0.05   1.04   0.08   0.43   0.88   1.62   3.85    533   1.01\n",
      "lambda[21]   1.19    0.04   1.04   0.11   0.46   0.88   1.61    3.9    865    1.0\n",
      "lambda[22]   0.78    0.04   0.88   0.04    0.2   0.47   1.05   3.25    428    1.0\n",
      "lambda[23]   0.91    0.04   0.94   0.05   0.27    0.6   1.21   3.53    442   1.01\n",
      "lambda[24]   0.83    0.05   0.92   0.03    0.2    0.5   1.12   3.34    359   1.01\n",
      "lambda[25]   1.22    0.04   1.03   0.12   0.48   0.93   1.64   3.96    750   1.01\n",
      "lambda[26]   1.18    0.04   1.06   0.08   0.43   0.86   1.62   4.06    634   1.01\n",
      "lambda[27]   1.06    0.04   0.97   0.08   0.37   0.77   1.44   3.77    603   1.01\n",
      "lambda[28]   1.13    0.04   1.02   0.09   0.42   0.81   1.54   3.88    636   1.01\n",
      "lambda[29]   1.09    0.04   1.03   0.08   0.38   0.78   1.45   3.94    657   1.01\n",
      "lambda[30]   1.18    0.04   1.01    0.1   0.47   0.89   1.58   3.94    751    1.0\n",
      "lambda[31]   0.89    0.05   0.92   0.05   0.26   0.58   1.21   3.36    369   1.01\n",
      "lambda[32]   0.77    0.05   0.86   0.05   0.21   0.47   1.01    3.2    334   1.01\n",
      "lambda[33]   0.93    0.05   1.02   0.04   0.23   0.58   1.26   3.68    382   1.02\n",
      "lambda[34]   1.21    0.04   1.04    0.1   0.46    0.9   1.64   3.92    824    1.0\n",
      "lambda[35]   1.23    0.04   1.03   0.11   0.49   0.94   1.67   4.01    569   1.01\n",
      "lambda[36]   1.07    0.04   0.95   0.07   0.38   0.79   1.48   3.58    623   1.01\n",
      "lambda[37]   1.08    0.04   1.01   0.07   0.36   0.78   1.49   3.79    593    1.0\n",
      "lambda[38]   1.19    0.04   1.03    0.1   0.46    0.9   1.62   3.92    792   1.01\n",
      "lambda[39]   0.92    0.04   0.94   0.06   0.29   0.61   1.23   3.51    530   1.01\n",
      "lambda[40]   1.22    0.04   1.03    0.1   0.48   0.93   1.66   3.98    781    1.0\n",
      "lambda[41]   1.15    0.03   1.01    0.1   0.44   0.84   1.54    3.9    846    1.0\n",
      "lambda[42]   1.16    0.04   1.03   0.09   0.43   0.85   1.58    3.9    663   1.01\n",
      "lambda[43]   1.14    0.04   1.02   0.08   0.43   0.84   1.54   3.83    574    1.0\n",
      "lambda[44]   1.22    0.04   1.03   0.11   0.48   0.93   1.66    4.0    769    1.0\n",
      "lambda[45]    1.0    0.04   0.94   0.08   0.35   0.71   1.33   3.57    706    1.0\n",
      "lambda[46]   0.93    0.04   0.96   0.05   0.27   0.61   1.27   3.54    501   1.01\n",
      "lambda[47]   1.22    0.04   1.05    0.1   0.48   0.93   1.65   3.99    836    1.0\n",
      "lambda[48]    1.1    0.04   1.02   0.09    0.4    0.8   1.47   3.94    654   1.01\n",
      "lambda[49]   1.24    0.04   1.07   0.11   0.49   0.93   1.67   4.08    698   1.01\n",
      "lambda[50]   1.18    0.04   1.08   0.09   0.43   0.87   1.62   4.05    760    1.0\n",
      "lambda[51]   1.09    0.05   1.02   0.06   0.35   0.78   1.49    3.8    414   1.01\n",
      "lambda[52]   1.09    0.04   0.96   0.08   0.41   0.81   1.48   3.66    545   1.01\n",
      "lambda[53]   1.21    0.04   1.04   0.11   0.46    0.9   1.64   3.95    856    1.0\n",
      "lambda[54]   0.72    0.05   0.89   0.03   0.16    0.4   0.95   3.19    344   1.01\n",
      "lambda[55]   1.11    0.04   1.05   0.08   0.37   0.78    1.5   4.02    617   1.01\n",
      "lambda[56]   1.17    0.03   1.02    0.1   0.43   0.88   1.57   3.83    877    1.0\n",
      "lambda[57]   1.28    0.04   1.07   0.11   0.51   0.99   1.74   4.09    602   1.01\n",
      "lambda[58]   1.18    0.04    1.0   0.11   0.47   0.89   1.58   3.83    781    1.0\n",
      "lambda[59]   0.06  2.7e-3   0.07 5.6e-3   0.02   0.04   0.07    0.2    620   1.01\n",
      "lambda[60]   1.03    0.05    1.0   0.07   0.34   0.72    1.4   3.73    443   1.01\n",
      "lambda[61]   1.17    0.04   0.99    0.1   0.46   0.89   1.58   3.86    789   1.01\n",
      "lambda[62]   1.19    0.04   1.02    0.1   0.47    0.9   1.61   3.89    776    1.0\n",
      "lambda[63]   1.07    0.03   0.95   0.09   0.41   0.79   1.44   3.62    775   1.01\n",
      "lambda[64]   0.61    0.06   0.84   0.02    0.1   0.26   0.76   2.96    182   1.03\n",
      "lambda[65]   1.05    0.04   0.97   0.08   0.38   0.76   1.41   3.63    700   1.01\n",
      "lambda[66]    1.1    0.04    1.0   0.08    0.4   0.81   1.51   3.84    767    1.0\n",
      "lambda[67]   1.16    0.04   1.03    0.1   0.43   0.86   1.56   3.95    668    1.0\n",
      "lambda[68]   0.32    0.02   0.38   0.03   0.11   0.21   0.39   1.41    279   1.02\n",
      "lambda[69]   1.23    0.04   1.06    0.1   0.47   0.93   1.67   4.11    645   1.01\n",
      "lambda[70]   1.11    0.04   0.99   0.09   0.42   0.82    1.5   3.77    638   1.01\n",
      "lambda[71]   0.06  2.4e-3   0.06 6.6e-3   0.03   0.05   0.08   0.22    743    1.0\n",
      "lambda[72]   1.19    0.03   1.02   0.12   0.49    0.9   1.59   3.92    899    1.0\n",
      "lambda[73]   1.17    0.04   1.03   0.09   0.45   0.88   1.59   3.87    691   1.01\n",
      "lambda[74]   0.09  3.7e-3   0.09 8.2e-3   0.03   0.06   0.11    0.3    615   1.01\n",
      "lambda[75]   1.16    0.04   1.01   0.09   0.43   0.86   1.56   3.92    757    1.0\n",
      "lambda[76]   1.21    0.04   1.05    0.1   0.47    0.9   1.63   4.05    683    1.0\n",
      "lambda[77]   1.15    0.04   1.02    0.1   0.43   0.85   1.54   3.88    833    1.0\n",
      "lambda[78]   1.15    0.04   1.03   0.08   0.42   0.85   1.54   3.92    651   1.01\n",
      "lambda[79]   1.15    0.04    1.0   0.09   0.44   0.87   1.58   3.81    774   1.01\n",
      "lambda[80]   1.13    0.04   1.02    0.1   0.41   0.82   1.53   3.88    674   1.01\n",
      "lambda[81]   0.05  3.0e-3   0.07 3.7e-3   0.02   0.04   0.07    0.2    506    1.0\n",
      "lambda[82]   0.93    0.05   0.93   0.06   0.29   0.62   1.26   3.41    379   1.01\n",
      "lambda[83]   1.07    0.05   1.02   0.06   0.35   0.76   1.44   3.86    502   1.01\n",
      "lambda[84]   0.96    0.03    0.9   0.07   0.33   0.67    1.3   3.44    684   1.01\n",
      "lambda[85]   0.22    0.02   0.37   0.01   0.06   0.12   0.24   1.15    246   1.02\n",
      "lambda[86]   1.11    0.04   0.98    0.1   0.41   0.82   1.49   3.73    744    1.0\n",
      "lambda[87]   1.15    0.04   0.99    0.1   0.45   0.87   1.54   3.73    794    1.0\n",
      "lambda[88]   1.05    0.04   0.99   0.07   0.36   0.75   1.44   3.69    635    1.0\n",
      "lambda[89]   1.04    0.04   0.98   0.06   0.35   0.76    1.4   3.72    553   1.01\n",
      "lambda[90]   1.06    0.04    1.0   0.06   0.36   0.75   1.44   3.73    606   1.01\n",
      "lambda[91]   1.19    0.04   1.03   0.09   0.45   0.89   1.62   3.97    688    1.0\n",
      "lambda[92]   0.18    0.02   0.29 9.7e-3   0.05   0.11    0.2   0.82    300   1.01\n",
      "lambda[93]   1.15    0.03   0.99   0.11   0.46   0.86   1.53   3.78    855    1.0\n",
      "lambda[94]    1.1    0.03   0.96   0.09   0.41   0.83   1.47   3.73    809    1.0\n",
      "lambda[95]    1.2    0.04   1.05   0.09   0.44    0.9   1.63   3.92    677   1.01\n",
      "lambda[96]    1.2    0.04   1.05    0.1   0.45   0.89   1.63   4.06    692   1.01\n",
      "lambda[97]   1.16    0.05   1.03    0.1   0.43   0.85   1.57   3.99    489   1.01\n",
      "lambda[98]   1.13    0.04   1.01   0.09   0.41   0.83   1.53   3.85    706   1.01\n",
      "lambda[99]   1.15    0.04   0.99    0.1   0.46   0.87   1.56   3.79    741    1.0\n",
      "tau[1]      34.14    9.95 263.98   0.03   0.41   1.79   8.53 210.71    704   1.01\n",
      "tau[2]      32.38    7.04 213.43   0.03   0.47   2.11   9.81 241.94    919    1.0\n",
      "tau[3]       9.98    1.77  69.86   0.02   0.33   1.18   4.76  67.44   1565    1.0\n",
      "tau[4]      26.78    13.1 437.67   0.02   0.29   1.08   4.32  94.83   1116   1.01\n",
      "tau[5]     173.59   82.12 2380.6   0.03   0.68   3.48  17.82 428.52    840    1.0\n",
      "tau[6]      74.63   37.56  688.0   0.04   0.63   3.18  14.96 283.13    336   1.01\n",
      "tau[7]      22.66    6.28 249.62   0.02   0.38   1.55   6.49 126.32   1579    1.0\n",
      "tau[8]      17.56    6.01 209.75   0.01   0.33   1.34   5.35  89.84   1217    1.0\n",
      "tau[9]       61.3   28.68 1062.7   0.02   0.31   1.27   5.48 128.61   1373    1.0\n",
      "tau[10]     17.48    2.47   76.3   0.02   0.34   1.62   7.73 140.48    957    1.0\n",
      "tau[11]     87.09   25.98 1023.8   0.03   0.68   3.61  18.83 353.36   1553    1.0\n",
      "tau[12]     32.42    7.65 230.74   0.02   0.43   1.89    8.9 232.53    910    1.0\n",
      "tau[13]     12.95    1.96  58.93   0.02    0.3   1.15   5.42 107.17    908   1.01\n",
      "tau[14]    157.59  140.69 4787.6   0.02   0.46   2.01   8.76 141.71   1158    1.0\n",
      "tau[15]    731.33  674.54  1.7e4   0.04   0.76   3.61  16.98 344.85    662   1.01\n",
      "tau[16]    5516.6  987.56  2.5e4  67.21 414.63 1027.6 2814.0  3.8e4    618    1.0\n",
      "tau[17]      90.5   22.93  784.1   0.04   0.73   4.44   26.9 547.19   1169   1.01\n",
      "tau[18]    605.07  221.09 5909.9   0.11   5.46  48.95 235.34 2951.9    715    1.0\n",
      "tau[19]     40.84   18.29  614.9   0.02   0.36   1.59   7.93 178.68   1130    1.0\n",
      "tau[20]     50.85   21.85 775.25   0.02   0.34   1.49   7.08 156.05   1259    1.0\n",
      "tau[21]     15.19    3.44  139.3   0.02   0.34   1.52   5.89  96.89   1638    1.0\n",
      "tau[22]    137.94   31.48 1060.4   0.05   1.12   7.29  41.64 880.26   1135    1.0\n",
      "tau[23]     77.69   18.86 660.29   0.03   0.76   3.94  21.17 484.08   1226    1.0\n",
      "tau[24]    286.07  102.24 3934.6   0.04   0.91   6.29  42.91 1121.3   1481    1.0\n",
      "tau[25]     11.16    1.81  61.32   0.02   0.34   1.37   5.47  78.76   1154   1.01\n",
      "tau[26]     36.88   13.89 340.39   0.01   0.35   1.63    7.1 200.32    600    1.0\n",
      "tau[27]     31.16    7.34 357.63   0.03    0.5   2.08  10.29 190.98   2375    1.0\n",
      "tau[28]     21.76    4.66 130.91   0.02   0.42   1.78   7.93 156.14    789    1.0\n",
      "tau[29]     28.92    5.53 208.16   0.02   0.47   2.06    9.6 195.61   1415    1.0\n",
      "tau[30]     51.46   28.86 970.46   0.03   0.38   1.55   5.82 105.85   1130   1.01\n",
      "tau[31]    349.04  286.91  1.3e4   0.04    0.8   4.59  24.97 466.07   2161    1.0\n",
      "tau[32]     78.53   12.89 402.98   0.05   1.22   7.62  39.24 551.09    978    1.0\n",
      "tau[33]    112.49   25.62 678.24   0.02   0.69   4.03  29.58 951.61    701   1.01\n",
      "tau[34]     24.03    6.56 226.02   0.02   0.32   1.38   5.84 135.13   1188    1.0\n",
      "tau[35]     21.45    8.14 211.71   0.02   0.31   1.25   5.14 108.38    676   1.01\n",
      "tau[36]     31.32    7.58 226.49   0.04   0.48   1.98   9.05 201.37    893    1.0\n",
      "tau[37]      48.7   16.77 614.07   0.02   0.44   2.09  10.63 239.73   1340    1.0\n",
      "tau[38]     22.34    8.11 219.53   0.03   0.34   1.37   6.21 110.75    732   1.01\n",
      "tau[39]     44.93    8.34 244.55   0.03   0.72   3.92   18.8 342.15    860    1.0\n",
      "tau[40]      20.6    5.93 165.71   0.03   0.33   1.25   5.31 132.85    781    1.0\n",
      "tau[41]     23.04    5.74 170.69   0.02   0.41   1.61   6.85 125.06    883    1.0\n",
      "tau[42]     25.08    8.13 407.47   0.02   0.37   1.63   7.18  126.3   2509    1.0\n",
      "tau[43]     46.04   23.09 530.14   0.02   0.38   1.62   7.41 187.97    527   1.01\n",
      "tau[44]     23.74    9.45  251.9   0.02   0.34   1.33   5.52 114.68    710    1.0\n",
      "tau[45]      23.7    3.27 113.15   0.03   0.61   2.75  11.63 167.86   1199    1.0\n",
      "tau[46]    110.34   33.25 1056.4   0.03   0.71   3.65  22.06 631.52   1009    1.0\n",
      "tau[47]     14.27    1.89  71.16   0.03   0.33   1.31   5.53 109.25   1424    1.0\n",
      "tau[48]     33.81   11.59 395.57   0.02   0.44    2.1   8.58 141.74   1166    1.0\n",
      "tau[49]     14.99    3.28 102.09   0.02   0.33   1.31    5.3 104.19    971   1.01\n",
      "tau[50]     53.94    28.3 1071.2   0.02   0.35   1.51   7.23 167.04   1432    1.0\n",
      "tau[51]     95.84   49.23 1386.3   0.02   0.46   2.15  11.62 292.59    793    1.0\n",
      "tau[52]    217.11  166.32 4897.1   0.03   0.46   1.87   8.02 187.02    867   1.01\n",
      "tau[53]      14.8    2.94  110.5   0.02   0.33   1.36   6.05  92.05   1409    1.0\n",
      "tau[54]    187.69   32.69 1182.5   0.06   1.41  10.64  68.07 1318.5   1309   1.01\n",
      "tau[55]     86.59    56.8 2401.5   0.02   0.41   2.12   10.3 215.94   1788    1.0\n",
      "tau[56]     18.77    5.09 163.59   0.03   0.37   1.56   6.68 125.04   1033    1.0\n",
      "tau[57]     20.74     7.4 253.06   0.02   0.29   1.16   4.76  89.66   1170    1.0\n",
      "tau[58]      21.0    5.65 227.99   0.02   0.37    1.4   5.43 100.12   1626    1.0\n",
      "tau[59]    6835.4  1012.6  3.7e4  89.62 487.45 1250.1 3620.7  4.4e4   1319    1.0\n",
      "tau[60]     31.55    5.64 152.47   0.03   0.51   2.48  12.66 257.16    730   1.01\n",
      "tau[61]     22.06    7.42  331.7   0.02   0.39   1.49   6.31 133.91   2000    1.0\n",
      "tau[62]    260.74  216.57  1.0e4   0.02   0.37   1.44   5.42 122.63   2236    1.0\n",
      "tau[63]     28.57    9.28 307.86   0.03   0.51   2.01    8.0 158.43   1099    1.0\n",
      "tau[64]    629.18  168.87 5084.9   0.05   2.46  29.27 194.49 3442.4    907    1.0\n",
      "tau[65]     28.25    5.67 248.52   0.04   0.53   2.17   9.45 175.46   1922    1.0\n",
      "tau[66]     27.45    5.47 166.16   0.03   0.41   1.82   8.84 201.58    923    1.0\n",
      "tau[67]     17.35    2.86   90.6   0.02   0.41   1.62   6.59 120.34   1005    1.0\n",
      "tau[68]    289.45   59.42 1997.2    0.7  14.67  44.13 137.87 1702.5   1130    1.0\n",
      "tau[69]     35.97   16.14 582.54   0.02   0.33   1.32   5.57 117.47   1303    1.0\n",
      "tau[70]     32.38    9.43 301.14   0.03   0.45   1.82   7.41 127.27   1020   1.01\n",
      "tau[71]    5366.9  1413.8  6.2e4  71.11 367.99 900.92 2587.0  2.6e4   1901    1.0\n",
      "tau[72]     10.81    1.82  61.65   0.02   0.34   1.39    5.3  73.76   1150    1.0\n",
      "tau[73]    274.51  248.22 8889.0   0.02    0.4   1.57   6.36  145.0   1282    1.0\n",
      "tau[74]    8345.8  5888.3  2.0e5  32.23 196.29 538.11 1474.6  1.7e4   1171    1.0\n",
      "tau[75]     25.87    7.62  330.3   0.02    0.4   1.64   7.25 142.78   1878    1.0\n",
      "tau[76]      36.4   13.43 517.85   0.02   0.35   1.38   5.54 116.02   1487    1.0\n",
      "tau[77]     36.15   22.92 582.52   0.02   0.42   1.69   6.85 115.09    646   1.01\n",
      "tau[78]     30.72    8.53 263.98   0.02   0.39   1.64   7.46 161.04    957    1.0\n",
      "tau[79]     27.92    6.95 249.96   0.02    0.4   1.56   6.82 166.18   1292    1.0\n",
      "tau[80]     20.27    4.41 172.38   0.02   0.41   1.77   8.49 132.05   1527    1.0\n",
      "tau[81]     1.5e4  3442.3  1.0e5  74.21  570.9 1601.8 4790.2  9.5e4    876   1.01\n",
      "tau[82]    295.36  246.42 7290.2   0.04   0.69   3.48  17.26 348.38    875    1.0\n",
      "tau[83]    175.64   97.82 2248.9   0.03   0.47   2.29   12.6 342.09    529   1.01\n",
      "tau[84]     39.75    8.86 268.98   0.04   0.66   3.02  13.82 253.18    922    1.0\n",
      "tau[85]    997.87  163.48 6322.7   1.07  43.26 134.87 445.62 6159.8   1496    1.0\n",
      "tau[86]     23.98    8.26 238.88   0.02   0.43   1.81    8.1 134.77    836    1.0\n",
      "tau[87]     23.86    6.14 206.12   0.02   0.43   1.57   6.48 116.98   1127    1.0\n",
      "tau[88]    758.34  725.88  3.0e4   0.03   0.51   2.33  11.33 259.99   1708    1.0\n",
      "tau[89]    278.23   230.5 7226.0   0.03   0.52   2.24  11.28 248.86    983    1.0\n",
      "tau[90]     98.96   42.76 1484.8   0.03   0.48   2.24  10.86 347.44   1206    1.0\n",
      "tau[91]     18.96    3.27   97.2   0.02   0.35    1.4   6.16  154.8    883    1.0\n",
      "tau[92]    2709.7  874.45  3.3e4   2.62  57.76 176.93  600.7  1.3e4   1397    1.0\n",
      "tau[93]     18.51    5.64 218.79   0.03   0.43   1.59   5.92  94.37   1503    1.0\n",
      "tau[94]     30.69    8.59 391.44   0.04   0.46   1.83   7.73 146.15   2077    1.0\n",
      "tau[95]    666.76  638.94  1.7e4   0.03   0.35   1.46   6.57 146.68    729   1.01\n",
      "tau[96]     32.89   14.13  693.5   0.02   0.34   1.42   6.54 112.53   2409    1.0\n",
      "tau[97]     21.86    6.45  180.7   0.02   0.37   1.67   7.25 116.09    784    1.0\n",
      "tau[98]     34.55   14.44 474.99   0.02   0.42   1.83   7.57 138.22   1082    1.0\n",
      "tau[99]     15.94    2.44  96.76   0.02    0.4   1.55   6.51  115.8   1567    1.0\n",
      "sigma2       0.09    0.01   0.15   0.01   0.03   0.06   0.12   0.32    218   1.02\n",
      "lp__       -336.6    4.44  33.49 -400.0 -358.2 -336.8 -314.6 -271.5     57   1.08\n",
      "\n",
      "Samples were drawn using HMC at Mon Mar 25 17:01:34 2019.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "fit_gdP = sm_gdP.sampling(data=data, \n",
    "                          thin=1, \n",
    "                          control=control, \n",
    "                          n_jobs=4, \n",
    "                          init=\"random\", \n",
    "                          iter=2500, \n",
    "                          algorithm=\"HMC\", \n",
    "                          warmup=0)\n",
    "print(round((time()-start)/60, 2), \"mins to run\")\n",
    "print(fit_gdP);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean effective sample size: 963.3\n"
     ]
    }
   ],
   "source": [
    "a = fit_gdP.summary()[\"summary\"]\n",
    "ess = a[:,-2]\n",
    "print(\"Mean effective sample size:\", np.round(np.mean(ess),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace = fit_gdP.extract()\n",
    "xi_samples = trace[\"xi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(xi_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cover = np.zeros(d)\n",
    "ci = np.zeros((d,2))\n",
    "for i in range(d) :\n",
    "    ci[i,:] = np.percentile(xi_samples[:,i], q=[2.5, 97.5])\n",
    "    cover[i] = (ci[i,0]<両_true[i])&(両_true[i]<ci[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+IAAAEyCAYAAAB+u2pNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDQniAAAA0JAgDgAAAA39fxw7T9A0U0spAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1224x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig= plt.figure(figsize=(17,5))\n",
    "for i in range(d) :\n",
    "    plt.plot([i+1,i+1], ci[i], color=\"blue\", lw=1.5)\n",
    "plt.plot(np.arange(1,d+1), 両_true, \"ro\", markersize=4)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*np.mean(cover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
